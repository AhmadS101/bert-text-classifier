{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diNKnuz2cpWt",
        "outputId": "2d54912d-e522-4c2f-91c1-edbdb87e3913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "La47SXXnckGo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModel\n",
        "from tqdm import tqdm\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDVx6-eleY0B"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Configuration\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LENGTH = 512\n",
        "DROPOUT_RATE = 0.3\n",
        "FREEZE_LAYERS = True\n",
        "FREEZE_EMBEDDINGS = True\n",
        "FREEZE_EARLY_LAYERS = 6\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "EPOCHS = 3\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "\n",
        "VALIDATION_SPLIT = 0.2\n",
        "DATASET_NAME = \"fancyzhx/ag_news\"\n",
        "TEXT_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT_PATH = '/content/sample_data/checkpoints/BERT_Text_Classifier.pth'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YzxC_sQAeaFn"
      },
      "outputs": [],
      "source": [
        "# custome classification dataset\n",
        "class ClfDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "    self.texts= texts\n",
        "    self.labels= labels\n",
        "    self.tokenizer= tokenizer\n",
        "    self.max_length= max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.texts[idx])\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    encoding = self.tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=self.max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'labels': torch.tensor(label, dtype= torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EeCcvyLweaCc"
      },
      "outputs": [],
      "source": [
        "# tokenizer setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "L_WmEqAweZ_C"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_ag_news_dataset():\n",
        "  try:\n",
        "      # load dataset\n",
        "      dataset = load_dataset('fancyzhx/ag_news')\n",
        "      train_ds = dataset['train']\n",
        "      test_ds = dataset['test']\n",
        "\n",
        "      # convert to dataframe\n",
        "      train_df = pd.DataFrame(train_ds)\n",
        "      test_df = pd.DataFrame(test_ds)\n",
        "\n",
        "      # Create id2label and label2id mappings\n",
        "      label_feature = dataset['train'].features['label']\n",
        "      label_names = label_feature.names\n",
        "      id2label = {i: name for i, name in enumerate(label_names)}\n",
        "      label2id = {name: i for i, name in enumerate(label_names)}\n",
        "\n",
        "      # printing some infos\n",
        "      print('Dataset loaded successfully')\n",
        "      print(f'Training samples length: {len(train_df)}')\n",
        "      print(f'Test samples length: {len(test_df)}')\n",
        "      print(f'Dataset columns: {train_df.columns.tolist()}')\n",
        "      print(f'Label mappings: {id2label}')\n",
        "      print(f'First samples:\\n{train_df.head(2)}')\n",
        "\n",
        "      return train_df, test_df, id2label, label2id\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading dataset: {e}\")\n",
        "      return None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OInCDhpleZ1a",
        "outputId": "990281ff-9d51-479e-bebc-509f947104a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n",
            "Training samples length: 120000\n",
            "Test samples length: 7600\n",
            "Dataset columns: ['text', 'label']\n",
            "Label mappings: {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
            "First samples:\n",
            "                                                text  label\n",
            "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
            "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n"
          ]
        }
      ],
      "source": [
        "train_df, test_df, id2label, label2id = load_ag_news_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mUbMAYLweZyP"
      },
      "outputs": [],
      "source": [
        "# prepare dataloader\n",
        "def prepare_dataloader():\n",
        "  texts = train_df[TEXT_COLUMN].tolist()\n",
        "  labels = train_df[LABEL_COLUMN].tolist()\n",
        "\n",
        "  train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED)\n",
        "\n",
        "  train_dataset = ClfDataset(train_texts, train_labels, tokenizer)\n",
        "  val_dataset = ClfDataset(train_texts, train_labels, tokenizer)\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "  return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "FB8ydqefeZvN"
      },
      "outputs": [],
      "source": [
        "train_dataloader, val_dataloader = prepare_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PeA2wJ-CVkn",
        "outputId": "3b9665ba-1bde-4040-a215-474f45e4e69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch input_ids shape: torch.Size([32, 512])\n",
            "Batch attention_mask shape: torch.Size([32, 512])\n",
            "Batch labels shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# check dataloaders\n",
        "for batch in train_dataloader:\n",
        "  print(f\"Batch input_ids shape: {batch['input_ids'].shape}\")\n",
        "  print(f\"Batch attention_mask shape: {batch['attention_mask'].shape}\")\n",
        "  print(f\"Batch labels shape: {batch['labels'].shape}\")\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qICusLswCVhx"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, model_name= None, num_classes=None, dropout_rate=None):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    model_name = model_name or MODEL_NAME\n",
        "    dropout_rate = dropout_rate or DROPOUT_RATE\n",
        "\n",
        "    self.bert = AutoModel.from_pretrained(model_name)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    if FREEZE_LAYERS:\n",
        "      self.freeze_layers()\n",
        "\n",
        "  def freeze_layers(self):\n",
        "    if FREEZE_EMBEDDINGS:\n",
        "      for param in self.bert.embeddings.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "    if FREEZE_EARLY_LAYERS > 0:\n",
        "      for layer in self.bert.encoder.layer[:FREEZE_EARLY_LAYERS]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    pooled_output = self.dropout(pooled_output)\n",
        "    logits = self.classifier(pooled_output)\n",
        "    return logits\n",
        "\n",
        "\n",
        "  def get_model_info(self):\n",
        "    total_params = sum(p.numel() for p in self.parameters())\n",
        "    trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    return {\n",
        "      'total_params': total_params,\n",
        "      'trainable_params': trainable_params,\n",
        "      'frozen_params': total_params - trainable_params\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GTgas2XfCVew"
      },
      "outputs": [],
      "source": [
        "model = BERTClassifier(num_classes=len(label2id)).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIkEJnB7CVcB",
        "outputId": "7931ca3b-71cf-46c0-89bf-fbc620d27e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with 4 classes\n",
            "Total parameters: 109,485,316\n",
            "Trainable parameters: 43,120,900\n",
            "Frozen parameters: 66,364,416\n"
          ]
        }
      ],
      "source": [
        "info = model.get_model_info()\n",
        "print(f\"Model created with {len(label2id)} classes\")\n",
        "print(f\"Total parameters: {info['total_params']:,}\")\n",
        "print(f\"Trainable parameters: {info['trainable_params']:,}\")\n",
        "print(f\"Frozen parameters: {info['frozen_params']:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fuqU4p0oU81h"
      },
      "outputs": [],
      "source": [
        "\n",
        "# initialize optimizer, scheduler, loss funcrion\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr = LEARNING_RATE,\n",
        "    weight_decay = WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = int(WARMUP_RATIO * total_steps),\n",
        "    num_training_steps = total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8sPbWX8aNzwQ"
      },
      "outputs": [],
      "source": [
        "# train one epoch\n",
        "def train_epoch(train_dataloader):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  train_progress = tqdm(train_dataloader, desc=\"Training\")\n",
        "  for batch in train_progress:\n",
        "    input_ids = batch['input_ids'].to(DEVICE)\n",
        "    attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "    labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    loss = loss_fn(logits, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    train_progress.set_postfix({'loss': loss.item()})\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  print(f\"Training Loss: {avg_loss}\")\n",
        "  return avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mcAFR2eqNztI"
      },
      "outputs": [],
      "source": [
        "# evaluation fun\n",
        "def evaluate(val_dataloader):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  all_predictions = []\n",
        "  all_labels = []\n",
        "  val_progress = tqdm(val_dataloader, desc=\"Validation\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_progress:\n",
        "      input_ids = batch['input_ids'].to(DEVICE)\n",
        "      attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "      labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "\n",
        "      logits = model(input_ids, attention_mask)\n",
        "      loss = loss_fn(logits, labels)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      val_progress.set_postfix({'loss': loss.item()})\n",
        "\n",
        "      prediction = torch.argmax(logits, dim=1)\n",
        "      all_predictions.extend(prediction.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  avg_loss = total_loss / len(val_dataloader)\n",
        "  accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
        "  print(f\"Validation Loss: {avg_loss}\")\n",
        "  print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "  return avg_loss, accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK47FmNbYgAD",
        "outputId": "9cd171f7-7ae7-4bdb-d20c-6e4ef93a97b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint found — starting from scratch.\n"
          ]
        }
      ],
      "source": [
        "def load_checkpoint(model, optimizer, filename):\n",
        "  if os.path.exists(filename):\n",
        "    checkpoint = torch.load(filename, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Checkpoint loaded — resuming from epoch {start_epoch}\")\n",
        "    return start_epoch\n",
        "  else:\n",
        "    print(\"No checkpoint found — starting from scratch.\")\n",
        "    return 0\n",
        "start_epoch = load_checkpoint(model, optimizer, CHECKPOINT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BhZtgveOkUcN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
        "  os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "  checkpoint = {\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss\n",
        "  }\n",
        "  torch.save(checkpoint, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KQyeRjanNzqE"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "def train(train_dataloader, val_dataloader, epochs=EPOCHS, start_epoch=0):\n",
        "  # Training history\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "  best_val_accuracy = 0\n",
        "\n",
        "  for epoch in range(start_epoch, epochs):\n",
        "    train_loss = train_epoch(train_dataloader)\n",
        "    val_loss, val_accuracy = evaluate(val_dataloader)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "      best_val_accuracy = val_accuracy\n",
        "      print(f\"Best validation accuracy: {best_val_accuracy}\")\n",
        "\n",
        "    save_checkpoint(model, optimizer, epoch, val_loss, CHECKPOINT_PATH)\n",
        "  print(f\"\\nTraining completed!\")\n",
        "  print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "  return {\n",
        "      'train_losses': train_losses,\n",
        "      'val_losses': val_losses,\n",
        "      'val_accuracies': val_accuracies,\n",
        "      'best_val_accuracy': best_val_accuracy\n",
        "  }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcD7aNJWkX0n",
        "outputId": "26fd828b-b734-4967-d76b-55bb741f4e98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  14%|█▍        | 422/3000 [12:56<1:19:19,  1.85s/it, loss=0.591]"
          ]
        }
      ],
      "source": [
        "training = train(train_dataloader, val_dataloader, epochs=EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2fx-sD1eArB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
